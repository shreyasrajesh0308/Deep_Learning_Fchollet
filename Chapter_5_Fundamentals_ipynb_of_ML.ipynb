{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter_5_Fundamentals.ipynb_of_ML",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN2pJy4JnQnYTFSTE23mFQR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyasrajesh0308/Deep_Learning/blob/master/Chapter_5_Fundamentals_ipynb_of_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GviOAE_9pu9P"
      },
      "source": [
        "# Fundamentals of Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsmuD3LTp8Xk"
      },
      "source": [
        "## Generalizing: The goal of Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw0yAJ9et7ZZ"
      },
      "source": [
        "Generalizing and optimization are in some ense rivals in machine learning. While Generalizing is trying to optimize for any dataset Optimization is focussed on reducing the loss for the train set. Hence in the training process we generally have 3 cycles.\n",
        "\n",
        "**Underfitting** : The model can still learn i.e here optimization and generalization are directly correlated since as the training loss decreases the validation loss also decreases.\n",
        "\n",
        "**Robust fit**: Optimzal point for both generalization and optimization, the validation loss is now stalling with the training loss.\n",
        "\n",
        "**Overfit**: At this point the validation loss is starting to increase as the training loss decreseas. Here the system is optimizing for the data but is not generalizing. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38Y7Wab4nh4u"
      },
      "source": [
        "Rare Features and Spurious correlations are two main factors that lead to overfitting. \n",
        "\n",
        "**Rare Features**: machine learning models trained on datasets that include rare feature values are highly susceptible to overfitting. In a sentiment classification task, if the word “cherimoya” (a fruit native to the Andes) only appears in one text in the train- ing data, and this text happens to be negative in sentiment, a poorly regularized model might put a very high weight on this word and always classify new texts that mention cherimoyas as negative, whereas, objectively, there’s nothing negative about the cherimoya.\n",
        "\n",
        "**Spurious Correlation**: If you’ve only ever seen two orange tabby cats in your life, and they both happened to be terribly antisocial, you might infer that orange tabby cats are generally likely to be antisocial. That’s overfitting: if you had been exposed to a wider variety of cats, includ- ing more orange ones, you’d have learned that cat color is not well correlated with character."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9ya0KIvoDUv"
      },
      "source": [
        "Importantly, a feature value doesn’t need to occur only a couple of times to lead to spurious correlations. Consider a word that occurs in 100 samples in your training data and that’s associated with a positive sentiment 54% of the time and with a nega- tive sentiment 46% of the time. That difference may well be a complete statistical fluke, yet your model is likely to learn to leverage that feature for its classification task. This is one of the most common sources of overfitting.\n",
        "\n",
        "\n",
        "Here’s a striking example. Take MNIST. Create a new training set by concatenating 784 white noise dimensions to the existing 784 dimensions of the data, so half of the data is now noise. For comparison, also create an equivalent dataset by concatenating 784 all-zeros dimensions. Our concatenation of meaningless features does not at all affect the information content of the data: we’re only adding something. Human clas- sification accuracy wouldn’t be affected by these transformations at all.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxujkmWdoHzB",
        "outputId": "a1860d35-bde0-41ce-9c48-01f08424af88"
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np \n",
        "\n",
        "(train_images, train_labels), _ = mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape((60000, 28*28))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "\n",
        "train_images_with_noise_channels = np.concatenate([train_images, np.random.random((len(train_images), 784))], axis=1)\n",
        "train_images_with_zeros_channels = np.concatenate([train_images, np.zeros((len(train_images), 784))], axis=1)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXsaW4ovqH2l"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model():\n",
        "\n",
        "  model = keras.Sequential([\n",
        "                            layers.Dense(512, activation = \"relu\"),\n",
        "                            layers.Dense(10, activation = \"softmax\")\n",
        "                          ])\n",
        "  \n",
        "  \n",
        "  model.compile(optimizer = \"rmsprop\", loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"])\n",
        "\n",
        "\n",
        "  return model"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEu09P_gqx2c",
        "outputId": "aed4a49c-268c-436b-a848-0bca0c9e6b7f"
      },
      "source": [
        "model = get_model() \n",
        "\n",
        "history_noise = model.fit(train_images_with_noise_channels, train_labels, epochs = 10, batch_size = 128, validation_split = 0.2)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "375/375 [==============================] - 4s 6ms/step - loss: 0.5985 - accuracy: 0.8156 - val_loss: 0.2898 - val_accuracy: 0.9110\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.2452 - accuracy: 0.9245 - val_loss: 0.1978 - val_accuracy: 0.9413\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1610 - accuracy: 0.9501 - val_loss: 0.1636 - val_accuracy: 0.9523\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1144 - accuracy: 0.9641 - val_loss: 0.1444 - val_accuracy: 0.9554\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0828 - accuracy: 0.9741 - val_loss: 0.1386 - val_accuracy: 0.9581\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0618 - accuracy: 0.9798 - val_loss: 0.1099 - val_accuracy: 0.9688\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0477 - accuracy: 0.9843 - val_loss: 0.1243 - val_accuracy: 0.9663\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0346 - accuracy: 0.9888 - val_loss: 0.1490 - val_accuracy: 0.9619\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.0256 - accuracy: 0.9918 - val_loss: 0.1396 - val_accuracy: 0.9653\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0204 - accuracy: 0.9934 - val_loss: 0.1404 - val_accuracy: 0.9648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfhj4wycrElL",
        "outputId": "a49e2c1f-b7d1-4c8c-ec7c-cae64a2db55a"
      },
      "source": [
        "model = get_model() \n",
        "\n",
        "history_zeros = model.fit(train_images_with_zeros_channels, train_labels, epochs = 10, batch_size = 128, validation_split = 0.2)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "375/375 [==============================] - 3s 6ms/step - loss: 0.2886 - accuracy: 0.9164 - val_loss: 0.1527 - val_accuracy: 0.9548\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1203 - accuracy: 0.9645 - val_loss: 0.1158 - val_accuracy: 0.9650\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0790 - accuracy: 0.9764 - val_loss: 0.0966 - val_accuracy: 0.9697\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0572 - accuracy: 0.9827 - val_loss: 0.0898 - val_accuracy: 0.9737\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0428 - accuracy: 0.9874 - val_loss: 0.0800 - val_accuracy: 0.9774\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0324 - accuracy: 0.9905 - val_loss: 0.0862 - val_accuracy: 0.9770\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0246 - accuracy: 0.9928 - val_loss: 0.0828 - val_accuracy: 0.9770\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0189 - accuracy: 0.9945 - val_loss: 0.0793 - val_accuracy: 0.9780\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0141 - accuracy: 0.9964 - val_loss: 0.0809 - val_accuracy: 0.9793\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.0106 - accuracy: 0.9969 - val_loss: 0.0898 - val_accuracy: 0.9792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En1D4RX_rVTM"
      },
      "source": [
        "Comparing Validation accuracy of both we have"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "JEmQ-tgOrgo6",
        "outputId": "e432bc70-39df-4956-ecac-0c699bc7a586"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "val_acc_noise = history_noise.history[\"val_accuracy\"]\n",
        "val_acc_zeros = history_zeros.history[\"val_accuracy\"]\n",
        "epochs = range(1, 11)\n",
        "plt.plot(epochs, val_acc_noise, \"b-\",label=\"Validation accuracy with noise channels\")\n",
        "plt.plot(epochs, val_acc_zeros, \"b--\",label=\"Validation accuracy with zeros channels\")\n",
        "plt.title(\"Effect of noise channels on validation accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fc5a0054890>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBfVkmkhriUn"
      },
      "source": [
        "Here, it is visible that there is about a 1% drop in the accuracy for the validation in the case of the random noise sample. The more noise channels you add, the further accuracy will degrade."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Irt9_VYoshrv"
      },
      "source": [
        "Noisy features inevitably lead to overfitting. As such, in cases where you aren’t sure whether the features you have are informative or distracting, it’s common to do feature selection before training. Restricting the IMDB data to the top 10,000 most common words was a crude form of feature selection, for instance. The typical way to do feature selection is to compute some usefulness score for each feature available—a measure of how informative the feature is with respect to the task, such as the mutual information between the feature and the labels—and only keep features that are above some threshold. Doing this would filter out the white noise channels in the preceding example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2ktd37wsw7k"
      },
      "source": [
        "#### The Nature of Generalization in Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIccmXV7tyWe"
      },
      "source": [
        "A remarkable fact about deep learning models is that they can be trained to fit anything, as long as they have enough representational power."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5t5Yve4uNs1"
      },
      "source": [
        "In the next example, we just shuffle the labels for the training data and we see that the training loss still goes on decreasing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOu4X9hKt3is",
        "outputId": "bce005b3-5b46-464e-bc5b-56d197f399c2"
      },
      "source": [
        "(train_images, train_labels) , _ = mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape((60000, 28*28))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "\n",
        "random_train_labels = train_labels[:]\n",
        "np.random.shuffle(random_train_labels)\n",
        "\n",
        "model = get_model()\n",
        "\n",
        "model.fit(train_images, random_train_labels,\n",
        "          epochs=100,\n",
        "          batch_size=128,\n",
        "          validation_split=0.2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 2.3170 - accuracy: 0.1009 - val_loss: 2.3073 - val_accuracy: 0.0975\n",
            "Epoch 2/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 2.3002 - accuracy: 0.1156 - val_loss: 2.3119 - val_accuracy: 0.0992\n",
            "Epoch 3/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 2.2914 - accuracy: 0.1281 - val_loss: 2.3196 - val_accuracy: 0.1019\n",
            "Epoch 4/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 2.2790 - accuracy: 0.1380 - val_loss: 2.3227 - val_accuracy: 0.1082\n",
            "Epoch 5/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 2.2611 - accuracy: 0.1519 - val_loss: 2.3398 - val_accuracy: 0.1007\n",
            "Epoch 6/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 2.2413 - accuracy: 0.1686 - val_loss: 2.3450 - val_accuracy: 0.1051\n",
            "Epoch 7/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 2.2184 - accuracy: 0.1789 - val_loss: 2.3630 - val_accuracy: 0.1015\n",
            "Epoch 8/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 2.1912 - accuracy: 0.1959 - val_loss: 2.3719 - val_accuracy: 0.1019\n",
            "Epoch 9/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 2.1627 - accuracy: 0.2110 - val_loss: 2.3967 - val_accuracy: 0.1079\n",
            "Epoch 10/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 2.1315 - accuracy: 0.2304 - val_loss: 2.4209 - val_accuracy: 0.1042\n",
            "Epoch 11/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 2.0989 - accuracy: 0.2450 - val_loss: 2.4398 - val_accuracy: 0.1045\n",
            "Epoch 12/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 2.0655 - accuracy: 0.2622 - val_loss: 2.4580 - val_accuracy: 0.0989\n",
            "Epoch 13/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 2.0311 - accuracy: 0.2773 - val_loss: 2.5044 - val_accuracy: 0.1047\n",
            "Epoch 14/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 1.9966 - accuracy: 0.2906 - val_loss: 2.5215 - val_accuracy: 0.1018\n",
            "Epoch 15/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 1.9625 - accuracy: 0.3061 - val_loss: 2.5466 - val_accuracy: 0.1020\n",
            "Epoch 16/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 1.9290 - accuracy: 0.3210 - val_loss: 2.5765 - val_accuracy: 0.1046\n",
            "Epoch 17/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 1.8938 - accuracy: 0.3340 - val_loss: 2.6090 - val_accuracy: 0.0999\n",
            "Epoch 18/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 1.8618 - accuracy: 0.3479 - val_loss: 2.6388 - val_accuracy: 0.1013\n",
            "Epoch 19/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 1.8287 - accuracy: 0.3612 - val_loss: 2.6993 - val_accuracy: 0.0992\n",
            "Epoch 20/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 1.7949 - accuracy: 0.3751 - val_loss: 2.7142 - val_accuracy: 0.1012\n",
            "Epoch 21/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 1.7642 - accuracy: 0.3868 - val_loss: 2.7455 - val_accuracy: 0.1047\n",
            "Epoch 22/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 1.7333 - accuracy: 0.3979 - val_loss: 2.7979 - val_accuracy: 0.1027\n",
            "Epoch 23/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 1.7016 - accuracy: 0.4101 - val_loss: 2.8194 - val_accuracy: 0.1000\n",
            "Epoch 24/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 1.6727 - accuracy: 0.4208 - val_loss: 2.8613 - val_accuracy: 0.1032\n",
            "Epoch 25/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 1.6445 - accuracy: 0.4304 - val_loss: 2.9268 - val_accuracy: 0.0986\n",
            "Epoch 26/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 1.6152 - accuracy: 0.4439 - val_loss: 2.9476 - val_accuracy: 0.1013\n",
            "Epoch 27/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 1.5862 - accuracy: 0.4534 - val_loss: 3.0030 - val_accuracy: 0.0993\n",
            "Epoch 28/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 1.5598 - accuracy: 0.4630 - val_loss: 3.0365 - val_accuracy: 0.1015\n",
            "Epoch 29/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 1.5316 - accuracy: 0.4734 - val_loss: 3.0732 - val_accuracy: 0.1045\n",
            "Epoch 30/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 1.5074 - accuracy: 0.4830 - val_loss: 3.1235 - val_accuracy: 0.1029\n",
            "Epoch 31/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 1.4829 - accuracy: 0.4911 - val_loss: 3.1536 - val_accuracy: 0.1066\n",
            "Epoch 32/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 1.4567 - accuracy: 0.5011 - val_loss: 3.1927 - val_accuracy: 0.1040\n",
            "Epoch 33/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 1.4320 - accuracy: 0.5107 - val_loss: 3.2450 - val_accuracy: 0.1035\n",
            "Epoch 34/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 1.4106 - accuracy: 0.5196 - val_loss: 3.2824 - val_accuracy: 0.1046\n",
            "Epoch 35/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 1.3868 - accuracy: 0.5282 - val_loss: 3.3564 - val_accuracy: 0.1032\n",
            "Epoch 36/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 1.3618 - accuracy: 0.5376 - val_loss: 3.4212 - val_accuracy: 0.1023\n",
            "Epoch 37/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 1.3396 - accuracy: 0.5433 - val_loss: 3.4401 - val_accuracy: 0.1030\n",
            "Epoch 38/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 1.3194 - accuracy: 0.5513 - val_loss: 3.4768 - val_accuracy: 0.1004\n",
            "Epoch 39/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 1.2967 - accuracy: 0.5611 - val_loss: 3.5550 - val_accuracy: 0.0979\n",
            "Epoch 40/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 1.2804 - accuracy: 0.5649 - val_loss: 3.5768 - val_accuracy: 0.1073\n",
            "Epoch 41/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 1.2585 - accuracy: 0.5729 - val_loss: 3.6231 - val_accuracy: 0.1038\n",
            "Epoch 42/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 1.2383 - accuracy: 0.5809 - val_loss: 3.6935 - val_accuracy: 0.1020\n",
            "Epoch 43/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 1.2175 - accuracy: 0.5896 - val_loss: 3.7291 - val_accuracy: 0.1005\n",
            "Epoch 44/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 1.1968 - accuracy: 0.5947 - val_loss: 3.7860 - val_accuracy: 0.1035\n",
            "Epoch 45/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 1.1774 - accuracy: 0.6025 - val_loss: 3.9042 - val_accuracy: 0.1012\n",
            "Epoch 46/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 1.1618 - accuracy: 0.6084 - val_loss: 3.8865 - val_accuracy: 0.0996\n",
            "Epoch 47/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 1.1441 - accuracy: 0.6135 - val_loss: 3.9155 - val_accuracy: 0.1037\n",
            "Epoch 48/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 1.1228 - accuracy: 0.6234 - val_loss: 4.0058 - val_accuracy: 0.1024\n",
            "Epoch 49/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 1.1110 - accuracy: 0.6247 - val_loss: 4.0501 - val_accuracy: 0.1010\n",
            "Epoch 50/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 1.0920 - accuracy: 0.6312 - val_loss: 4.0964 - val_accuracy: 0.1015\n",
            "Epoch 51/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 1.0728 - accuracy: 0.6416 - val_loss: 4.1432 - val_accuracy: 0.0978\n",
            "Epoch 52/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 1.0590 - accuracy: 0.6459 - val_loss: 4.2143 - val_accuracy: 0.0991\n",
            "Epoch 53/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 1.0440 - accuracy: 0.6488 - val_loss: 4.2877 - val_accuracy: 0.1009\n",
            "Epoch 54/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 1.0275 - accuracy: 0.6557 - val_loss: 4.3080 - val_accuracy: 0.1002\n",
            "Epoch 55/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 1.0115 - accuracy: 0.6622 - val_loss: 4.4198 - val_accuracy: 0.1041\n",
            "Epoch 56/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.9977 - accuracy: 0.6665 - val_loss: 4.4398 - val_accuracy: 0.1039\n",
            "Epoch 57/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.9810 - accuracy: 0.6717 - val_loss: 4.4859 - val_accuracy: 0.1014\n",
            "Epoch 58/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.9667 - accuracy: 0.6761 - val_loss: 4.5471 - val_accuracy: 0.0997\n",
            "Epoch 59/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.9519 - accuracy: 0.6832 - val_loss: 4.5738 - val_accuracy: 0.0998\n",
            "Epoch 60/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.9388 - accuracy: 0.6865 - val_loss: 4.6577 - val_accuracy: 0.0999\n",
            "Epoch 61/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.9243 - accuracy: 0.6938 - val_loss: 4.7426 - val_accuracy: 0.1004\n",
            "Epoch 62/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.9111 - accuracy: 0.6974 - val_loss: 4.7503 - val_accuracy: 0.0982\n",
            "Epoch 63/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.8982 - accuracy: 0.7018 - val_loss: 4.8168 - val_accuracy: 0.1018\n",
            "Epoch 64/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.8848 - accuracy: 0.7076 - val_loss: 4.9223 - val_accuracy: 0.0998\n",
            "Epoch 65/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.8719 - accuracy: 0.7107 - val_loss: 4.9746 - val_accuracy: 0.0991\n",
            "Epoch 66/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.8606 - accuracy: 0.7146 - val_loss: 4.9885 - val_accuracy: 0.1000\n",
            "Epoch 67/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.8479 - accuracy: 0.7169 - val_loss: 5.0883 - val_accuracy: 0.0997\n",
            "Epoch 68/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.8368 - accuracy: 0.7248 - val_loss: 5.0994 - val_accuracy: 0.0977\n",
            "Epoch 69/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.8237 - accuracy: 0.7274 - val_loss: 5.1538 - val_accuracy: 0.1007\n",
            "Epoch 70/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.8107 - accuracy: 0.7335 - val_loss: 5.2533 - val_accuracy: 0.0996\n",
            "Epoch 71/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.8011 - accuracy: 0.7368 - val_loss: 5.3380 - val_accuracy: 0.0984\n",
            "Epoch 72/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.7896 - accuracy: 0.7400 - val_loss: 5.4199 - val_accuracy: 0.0986\n",
            "Epoch 73/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.7783 - accuracy: 0.7420 - val_loss: 5.4513 - val_accuracy: 0.0988\n",
            "Epoch 74/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.7663 - accuracy: 0.7487 - val_loss: 5.4605 - val_accuracy: 0.1022\n",
            "Epoch 75/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.7550 - accuracy: 0.7521 - val_loss: 5.5572 - val_accuracy: 0.1007\n",
            "Epoch 76/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.7445 - accuracy: 0.7539 - val_loss: 5.6372 - val_accuracy: 0.1006\n",
            "Epoch 77/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.7339 - accuracy: 0.7601 - val_loss: 5.7008 - val_accuracy: 0.1002\n",
            "Epoch 78/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.7238 - accuracy: 0.7623 - val_loss: 5.7563 - val_accuracy: 0.0995\n",
            "Epoch 79/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.7152 - accuracy: 0.7658 - val_loss: 5.8043 - val_accuracy: 0.0988\n",
            "Epoch 80/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.7047 - accuracy: 0.7691 - val_loss: 5.8796 - val_accuracy: 0.1011\n",
            "Epoch 81/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.6947 - accuracy: 0.7709 - val_loss: 5.9256 - val_accuracy: 0.0988\n",
            "Epoch 82/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.6845 - accuracy: 0.7763 - val_loss: 6.0051 - val_accuracy: 0.0997\n",
            "Epoch 83/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.6749 - accuracy: 0.7802 - val_loss: 6.0640 - val_accuracy: 0.0989\n",
            "Epoch 84/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.6682 - accuracy: 0.7823 - val_loss: 6.1296 - val_accuracy: 0.0996\n",
            "Epoch 85/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.6573 - accuracy: 0.7854 - val_loss: 6.1861 - val_accuracy: 0.0981\n",
            "Epoch 86/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.6484 - accuracy: 0.7911 - val_loss: 6.2569 - val_accuracy: 0.1014\n",
            "Epoch 87/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.6378 - accuracy: 0.7930 - val_loss: 6.3312 - val_accuracy: 0.0966\n",
            "Epoch 88/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.6308 - accuracy: 0.7947 - val_loss: 6.3407 - val_accuracy: 0.1010\n",
            "Epoch 89/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.6207 - accuracy: 0.7990 - val_loss: 6.4011 - val_accuracy: 0.1016\n",
            "Epoch 90/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.6137 - accuracy: 0.7994 - val_loss: 6.4892 - val_accuracy: 0.0996\n",
            "Epoch 91/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.6062 - accuracy: 0.8027 - val_loss: 6.5776 - val_accuracy: 0.1004\n",
            "Epoch 92/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.5951 - accuracy: 0.8089 - val_loss: 6.6291 - val_accuracy: 0.0986\n",
            "Epoch 93/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.5895 - accuracy: 0.8095 - val_loss: 6.7140 - val_accuracy: 0.0953\n",
            "Epoch 94/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.5818 - accuracy: 0.8110 - val_loss: 6.7724 - val_accuracy: 0.0986\n",
            "Epoch 95/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.5745 - accuracy: 0.8125 - val_loss: 6.8654 - val_accuracy: 0.0983\n",
            "Epoch 96/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.5655 - accuracy: 0.8165 - val_loss: 6.9071 - val_accuracy: 0.0983\n",
            "Epoch 97/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.5600 - accuracy: 0.8181 - val_loss: 6.9725 - val_accuracy: 0.0982\n",
            "Epoch 98/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.5503 - accuracy: 0.8213 - val_loss: 7.0403 - val_accuracy: 0.0970\n",
            "Epoch 99/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.5445 - accuracy: 0.8228 - val_loss: 7.0843 - val_accuracy: 0.0977\n",
            "Epoch 100/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.5357 - accuracy: 0.8284 - val_loss: 7.1561 - val_accuracy: 0.0964\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc530f8ae90>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O8c1Y0Cu28f"
      },
      "source": [
        "As seen above the trianing accuracy in increasing but the validation obviously cant grow. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqQ0cCjBwtoH"
      },
      "source": [
        "In fact, you don’t even need to do this with MNIST data—you could just generate white noise inputs and random labels. You could fit a model on that, too, as long as it has enough parameters. It would just end up memorizing specific inputs, much like a Python dictionary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlAM5OvFxDo2"
      },
      "source": [
        "But if thats the case how do deep learning models generalize at all? should it be some form of mapping between training and labels like a dictionary.\n",
        "\n",
        "As it turns out, the nature of generalization in deep learning has rather little to do with deep learning models themselves, and much to do with the structure of information in the real world. Let’s take a look at what’s really going on here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emBkaEqPxa1_"
      },
      "source": [
        "#### The Manifold Hypothesis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OPzcBkbxeh8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}